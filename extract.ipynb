{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options as ChromeOptions\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChromeOptions to define specific features of that browser.\n",
    "chrome_options = ChromeOptions()\n",
    "# With \"--headless=new\" Selenium opens the browser minimized.\n",
    "chrome_options.add_argument(\"--headless=new\") \n",
    "# DataFrame model\n",
    "main_df = pd.DataFrame(columns = [\"Stadium\", \"Capacity\", \"Spectators\", \"Average\",\t\"Matches\", \"sold out\", \"Capacity\", \"Club\", \"Year\"]) \n",
    "# List of all seasons that should have their data extracted.\n",
    "seasons = [2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023]\n",
    "# List of leagues endpoints.\n",
    "leagues_prefixs = [\"SA1\", \"SA2L\"]\n",
    "dfs = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function extracts the Attendecens data using a loop in \"seasons\" list.\n",
    "\n",
    "def extract_attendences():\n",
    "  for i in range(len(seasons)): \n",
    "    # Each season is accessed with its respective index.\n",
    "    url = f'https://www.transfermarkt.com/saudi-pro-league/besucherzahlen/wettbewerb/SA1/saison_id/{seasons[i]}/plus/1'\n",
    "    # Here I start the Chrome driver, to access Selenium functionalities.\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.get(url)\n",
    "    # scrapingHTML function removes the table found on the website, transforming it into a DataFrame\n",
    "    html = scrapingHTML(driver, i)\n",
    "    print(f'Season {seasons[i]} - Scraping done!')\n",
    "\n",
    "  # After the loop. We join the dataframes and send them to the RAW layer as CSV file.\n",
    "  df_og = pd.concat(dfs)\n",
    "  df_og.to_csv(\"./RAW/ArabicSoccer/Attendances/Arabic_Attendances_2007_2023.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NoSuchElementExceptionHandler function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function was created to avoid code repetition.\n",
    "# Its purpose is to make the NoSuchElementException Handler.\n",
    "def NoSuchElementExceptionHandler(driver, ID, XPATH):\n",
    "  try:\n",
    "        WebDriverWait(driver, 10).until(EC.frame_to_be_available_and_switch_to_it(driver.find_element(By.ID, ID)))\n",
    "        driver.find_element(By.XPATH, XPATH).click()\n",
    "  except NoSuchElementException:\n",
    "    # If the error occurs and the cookie pop-up does not appear, driver.find_element is ignored.\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to get data with Selenium and BS4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapingHTML(driver, i):\n",
    "  # NoSuchElementException Handler.\n",
    "  NoSuchElementExceptionHandler(driver, \"sp_message_iframe_851946\", \"//*[@id='notice']/div[3]/div[3]/button\")\n",
    "  # After closing the pop-up, we search for the table using its XPATH.\n",
    "  table = driver.find_element(By.XPATH, '//*[@id=\"yw1\"]/table')\n",
    "  # Using the \"get_attribute\" method with the \"OuterHTML\" parameter we extract the HTML code from the table.  \n",
    "  data = table.get_attribute('outerHTML')\n",
    "  # Allowed to scroll through data in html. Then I use the find method to find the table tag.\n",
    "  soup = BeautifulSoup(data, 'html.parser')\n",
    "  html = soup.find(name='table')\n",
    "\n",
    "  # To transform HTML into DataFrame it is necessary to transform it into String as it is a BeautifulSoup Object.\n",
    "  # The pandas method \"read_html()\" returns a list of Dataframes, as we need the first one, I use [0].\n",
    "  df = pd.read_html(str(html))[0]\n",
    "  df[\"Club\"] = \"\"\n",
    "  # Create a column with the respective season.\n",
    "  df[\"Year\"] = seasons[i]\n",
    "  \n",
    "  # Insert the DataFrame into the list.\n",
    "  dfs.append(df)\n",
    "  # Close the navigator.\n",
    "  driver.quit()\n",
    "  return html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract Teams info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def teams_list_extract(league):\n",
    "  # Here we loop the list with league prefixes.\n",
    "  for i in range(len(league)):\n",
    "    url = f\"https://www.transfermarkt.us/saudi-pro-league/startseite/wettbewerb/{league[i]}\"\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.get(url)\n",
    "\n",
    "    NoSuchElementExceptionHandler(driver, \"sp_message_iframe_851946\", \"//*[@id='notice']/div[3]/div[3]/button\")\n",
    "    \n",
    "    table = driver.find_element(By.XPATH, '//*[@id=\"yw1\"]/table')\n",
    "    data = table.get_attribute('outerHTML')\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "    html = soup.find(name='table')\n",
    "\n",
    "    df = pd.read_html(str(html))[0]\n",
    "    # Create a column with the respective league name.\n",
    "    df[\"League\"] = league[i]\n",
    "\n",
    "    # Here the Dataframe is saved as CSV, in the RAW layer.\n",
    "    df.to_csv(f\"./RAW/ArabicSoccer/Teams/{league[i]}_teams_2023.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract All Champions - SPL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_champions(league):\n",
    "  # Here we loop the list with league prefixes.\n",
    "  for i in range(len(league)):\n",
    "    url = f\"https://www.transfermarkt.us/saudi-professional-league/erfolge/wettbewerb/{league[i]}\"\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.get(url)\n",
    "\n",
    "    NoSuchElementExceptionHandler(driver, \"sp_message_iframe_851946\", \"//*[@id='notice']/div[3]/div[3]/button\")\n",
    "    \n",
    "    table = driver.find_element(By.XPATH, '//*[@id=\"yw1\"]/table')\n",
    "    data = table.get_attribute('outerHTML')\n",
    "    soup = BeautifulSoup(data, 'html.parser')\n",
    "    html = soup.find(name='table')\n",
    "    df = pd.read_html(str(html))[0]\n",
    "    \n",
    "    # Entering the name of the \"League\" column, according to the loop index\n",
    "    # Considering that in the list leagues_prefixs = [\"SA1\", \"SA2L\"]\n",
    "    if i == 1:\n",
    "      # SA1 is Saudi Pro League\n",
    "      df[\"League\"] = \"Saudi Pro League\"\n",
    "    else:\n",
    "      # SA2l is Yelo League\n",
    "      df[\"League\"] = \"Yelo League\"\n",
    "\n",
    "     # Here the Dataframe is saved as CSV, in the RAW layer.\n",
    "    df.to_csv(f\"./RAW/ArabicSoccer/Champions/{league[i]}_Champions.csv\", encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing the functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Season 2007 - Scraping done!\n",
      "Season 2008 - Scraping done!\n",
      "Season 2009 - Scraping done!\n",
      "Season 2010 - Scraping done!\n",
      "Season 2011 - Scraping done!\n",
      "Season 2012 - Scraping done!\n",
      "Season 2013 - Scraping done!\n",
      "Season 2014 - Scraping done!\n",
      "Season 2015 - Scraping done!\n",
      "Season 2016 - Scraping done!\n",
      "Season 2017 - Scraping done!\n",
      "Season 2018 - Scraping done!\n",
      "Season 2019 - Scraping done!\n",
      "Season 2020 - Scraping done!\n",
      "Season 2021 - Scraping done!\n",
      "Season 2022 - Scraping done!\n",
      "Season 2023 - Scraping done!\n"
     ]
    }
   ],
   "source": [
    "extract_champions(leagues_prefixs)\n",
    "extract_attendences()\n",
    "teams_list_extract(leagues_prefixs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
